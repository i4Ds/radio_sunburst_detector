{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, GlobalAveragePooling2D\n",
    "from keras.models import Model,Dropout\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.applications import EfficientNetB0\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMERICAL VALUES(DIMENSIONS) WILL CHANGE\n",
    "#Connection of Input is required, connect to database and do train test validation splits\n",
    "#hp(hyperparameter) tuning is applied(hp.something(..)), but depending on performance, could do less or more parameters?\n",
    "#Possibly computationally expensive, could cut a layer from autoencoder, and be used later for feature extraction(type1,2...)\n",
    "#Decoder part of the autoencoder is commented out\n",
    "#CNN model used: efficientnet, could use simpler model for less use of computation resources, need to try\n",
    "#Use of transfer learning in CNN by unfreezing only the top 20 layers, could also go for the route of training the whole CNN with our data\n",
    "#Note that datasets will also change in the end: 1)data from one instrument, 2)data from another instrument, 3)hybrid dataset of these two => hp tuning for each one? too expensive?\n",
    "#No cross validation, probably inefficient\n",
    "#Use of a fine tuned(hp tuning), stacked(2 layers), sparse(encoder_l1 for regularization which leads to sparsity) autoencoder\n",
    "#Use of transformer would require design decisions regarding ML pipeline: Autoencoder,Transformer,CNN: Which one(s) to use and in which order? In any case, implementation of a transformer would be at later stages \n",
    "    #AE -> TF -> Classifier\n",
    "    #AE -> TF -> CNN -> Classifier\n",
    "    #AE -> CNN -> TF -> Classifier\n",
    "    #AE -> CNN -> Classifier\n",
    "#testing and analysis at the end\n",
    "#for hp tuning technique: instead of random search using hp, bahesian hyperparameter search using wandb\n",
    "#the maxpooling layer before \"encoder\" is reached is optional, try with and without: do hp tuning for both and compare their bests? NEED TO TRY.\n",
    "#for the second conv2d, should we use l1 regularization for sparcity again? Would this lead to overfitting and a more complex model or would it make it better? NEED TO TRY. In any case, do not tune that one, just give a constant value \n",
    "#Couldn't figure out how to save the best model???????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the data from database and preprocessing\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test, val split the data, maybe some preprocessing as well\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the random generator seeds for better reproducibility\n",
    "tf.random.set_seed(67)\n",
    "np.random.seed(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authorize wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can change to \"grid\" or \"random\"\n",
    "method = 'bayes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function to compile, build and train the model\n",
    "def train(): #give the data we pull from database to train function as parameter\n",
    "\n",
    "\n",
    "    #Default values for hyper-parameters\n",
    "    configs = {\n",
    "        'encoder_filters': 32,\n",
    "        'encoder_kernel_size': 3,\n",
    "        'encoder_l1': 0.00001,\n",
    "        'units': 32,\n",
    "        'dropout': 0.0,\n",
    "        'weight_initialization': 'glorot_uniform',\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.00001,\n",
    "        'method': method\n",
    "    }\n",
    "        \n",
    "   \n",
    "    #Initilize a new wandb run\n",
    "    wandb.init(project='automated radio wave spectrogram classifier', config=configs)\n",
    "        \n",
    "    #IMPORTANT\n",
    "    cnf = wandb.config\n",
    "\n",
    "    #Class to build the Model\n",
    "    class Model:\n",
    "        def __init__(self, input_shape, num_classes):\n",
    "            self.input_shape = input_shape\n",
    "            self.num_classes = num_classes\n",
    "        def build(self):\n",
    "\n",
    "        \n",
    "\n",
    "            input_img = Input(shape=self.input_shape)\n",
    "\n",
    "            # AUTOENCODER: how stacked should it be/How many layers? \n",
    "            #1st layer\n",
    "            x = Conv2D(\n",
    "                filters=cnf.encoder_filters,\n",
    "                kernel_size=cnf.encoder_kernel_size, #could be changed manually\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                activity_regularizer=regularizers.l1(cnf.encoder_l1),\n",
    "                kernel_initializer=cnf.weight_initialization\n",
    "            )(input_img)\n",
    "            x = MaxPooling2D((2, 2), padding='same')(x) #max pooling layer\n",
    "\n",
    "            #2nd layer\n",
    "            x = Conv2D(cnf.encoder_filters, (3, 3), activation='relu', padding='same')(x)\n",
    "            x = Conv2D(\n",
    "                filters=cnf.encoder_filters,\n",
    "                kernel_size=cnf.encoder_kernel_size,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                #activity_regularizer=regularizers.l1(cnf.encoder_l1),\n",
    "            )(x)\n",
    "            \n",
    "            x = MaxPooling2D((2, 2), padding='same')(x) #may not add, try both\n",
    "            #max pooling layer to give us the result of the encoding process: latent space\n",
    "            encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "            #start of decoding, remove comment for decoding\n",
    "            #x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "            #x = UpSampling2D((2, 2))(x)\n",
    "            #x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "            #x = UpSampling2D((2, 2))(x)\n",
    "            #decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "            #PRETRAINED EFFICIENTNET, could possibly use smaller but less computationaly expensive models: try,\n",
    "            # note that use of LSTM is computationally more expensive and is not sure to bring better accuracy but still worth trying \n",
    "            #also note the use of tranfer learning here, but could also go with the route of training the whole model: after HP tuning with validation data ofc\n",
    "            efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "\n",
    "            for layer in efficientnet.layers[:-20]:  #freezing everything except top 20 layers, again could also go for layer.trainable=true for all layers: do we have enough computational resources?\n",
    "                layer.trainable = False\n",
    "\n",
    "            x = efficientnet(encoded) #!!!!!!!!!!!!!!!!!!!!!!!change to encoded, change dimensions in other places accordingly\n",
    "            x = GlobalAveragePooling2D()(x) #helps reduce overfitting by reducing the total number of parameters in the model\n",
    "            x = Dense(cnf.units,kernel_initializer=cnf.weight_initialization)(x) #A fully connected dense layer, possibly for feature extracion, could think of adding more layers here for feature classifications in the future\n",
    "            x = Dropout(cnf.dropout)(x)#for regularization, again use of tuning: maybe too much?\n",
    "            output = Dense(self.num_classes, activation='softmax')(x)#fully connected dense layer with softmax activation for producing the output probabilities of classes\n",
    "                #This works as the classifier in our ML pipeline thanks to softmax\n",
    "\n",
    "\n",
    "            model = Model(inputs=input_img, outputs=output)\n",
    "            \n",
    "\n",
    "            model.compile(optimizer=cnf.optimizer, \n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy'],learning_rate=cnf.learning_rate)\n",
    "\n",
    "            return model\n",
    "        \n",
    "    s_model = Model(input_shape=(128, 128, 1) ,num_classes=2) #this part can be replaced with correct input shape\n",
    "    s_model = s_model.build()  \n",
    "\n",
    "    #Train the model\n",
    "    labels = [\"Yes\", \"No\"]\n",
    "    s_model.fit(\n",
    "        x_train, y_train, \n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=cnf.epochs,\n",
    "        batch_size=cnf.batch_size,\n",
    "        callbacks=[WandbCallback(data_type=\"image\", \n",
    "                validation_data=(X_test, y_test), labels = labels)] #?????????????? THE EXAMPLE CODE REQUIRES A LABELS VARIABLE HERE\n",
    "    )\n",
    "    \n",
    "    return s_model #this return is not very useful, when sweep operation takes place the last try will be the one kept, but still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to specify the tuning configuration, it would also return a sweep id (required for running the sweep)\n",
    "def get_sweep_id(method)\n",
    "    sweep_config = {\n",
    "    \"name\": \"sweep\",\n",
    "    \"method\": method,  \n",
    "    \"metric\":{\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"encoder_filters\": {\n",
    "            \"values\": [32, 64, 96, 128]\n",
    "        },\n",
    "        \"encoder_kernel_size\": {\n",
    "            \"values\": [3, 5]\n",
    "        },\n",
    "        \"encoder_l1\": {\n",
    "            \"min\": 0.00001,\n",
    "            \"max\": 0.01\n",
    "        },\n",
    "        \"weight_initialization\": {\n",
    "            \"values\": ['glorot_uniform', 'he_normal']\n",
    "        },\n",
    "        \"units\": {\n",
    "            \"min\": 32,\n",
    "            \"max\": 512\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"min\": 0.0,\n",
    "            \"max\": 0.5\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 0.00001,\n",
    "            \"max\": 0.01\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"min\": 32,\n",
    "            \"max\": 512\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": ['adam', 'sgd', 'rmsprop']\n",
    "        },\n",
    "    }\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project = \"automated radio wave spectrogram classifier\")\n",
    "    return sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sweep for bayes search\n",
    "sweep_id = get_sweep_id('bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the sweep\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing and Analysis: ROC curves, confusion matrices, accuracy scores... MAY NOT BE REQUIRED WITH WANDB\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A requirement when using notebook\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
