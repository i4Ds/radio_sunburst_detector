{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Class to build the Model\n",
    "class Model:\n",
    "        def __init__(self, input_shape, num_classes):\n",
    "            self.input_shape = input_shape\n",
    "            self.num_classes = num_classes\n",
    "        def build(self):\n",
    "\n",
    "        \n",
    "\n",
    "            input_img = Input(shape=self.input_shape)\n",
    "\n",
    "            # AUTOENCODER: how stacked should it be/How many layers? \n",
    "            #1st layer\n",
    "            x = Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=3, #could be changed manually\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                activity_regularizer=regularizers.l1(0.0001),\n",
    "                kernel_initializer='he_normal'\n",
    "            )(input_img)\n",
    "            x = MaxPooling2D((2, 2), padding='same')(x) #max pooling layer\n",
    "\n",
    "            #2nd layer\n",
    "            x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "            x = Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=3, #could be changed manually\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                activity_regularizer=regularizers.l1(0.0001),\n",
    "                kernel_initializer='he_normal',\n",
    "            )(x)\n",
    "            \n",
    "            x = MaxPooling2D((2, 2), padding='same')(x) #may not add, try both\n",
    "            #max pooling layer to give us the result of the encoding process: latent space\n",
    "            encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "            #start of decoding, remove comment for decoding\n",
    "            #x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "            #x = UpSampling2D((2, 2))(x)\n",
    "            #x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "            #x = UpSampling2D((2, 2))(x)\n",
    "            #decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "            #PRETRAINED EFFICIENTNET, could possibly use smaller but less computationaly expensive models: try,\n",
    "            # note that use of LSTM is computationally more expensive and is not sure to bring better accuracy but still worth trying \n",
    "            #also note the use of tranfer learning here, but could also go with the route of training the whole model: after HP tuning with validation data ofc\n",
    "            efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "\n",
    "            for layer in efficientnet.layers[:-20]:  #freezing everything except top 20 layers, again could also go for layer.trainable=true for all layers: do we have enough computational resources?\n",
    "                layer.trainable = False\n",
    "\n",
    "            x = efficientnet(encoded) #!!!!!!!!!!!!!!!!!!!!!!!change to encoded, change dimensions in other places accordingly\n",
    "            x = GlobalAveragePooling2D()(x) #helps reduce overfitting by reducing the total number of parameters in the model\n",
    "            x = Dense(32,kernel_initializer='glorot_uniform')(x) #A fully connected dense layer, possibly for feature extracion, could think of adding more layers here for feature classifications in the future\n",
    "            x = Dropout(0.3)(x)#for regularization, again use of tuning: maybe too much?\n",
    "            output = Dense(self.num_classes, activation='softmax')(x)#fully connected dense layer with softmax activation for producing the output probabilities of classes\n",
    "                #This works as the classifier in our ML pipeline thanks to softmax\n",
    "\n",
    "\n",
    "            model = Model(inputs=input_img, outputs=output)\n",
    "            \n",
    "\n",
    "            model.compile(optimizer='adam', \n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy'],learning_rate=0.0001)\n",
    "\n",
    "            return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_shape=(200, 200, 1) ,num_classes=2) #this part can be replaced with correct input shape\n",
    "model = model.build()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
