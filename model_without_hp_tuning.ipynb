{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to patch Tensorflow/Keras\n",
      "exception while trying to patch_tf_keras\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\wandb\\integration\\keras\\keras.py\", line 86, in patch_tf_keras\n",
      "    from keras.engine import training\n",
      "ModuleNotFoundError: No module named 'keras.engine'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to patch Tensorflow/Keras\n",
      "exception while trying to patch_tf_keras\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\wandb\\integration\\keras\\keras.py\", line 86, in patch_tf_keras\n",
      "    from keras.engine import training\n",
      "ModuleNotFoundError: No module named 'keras.engine'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to patch Tensorflow/Keras\n",
      "exception while trying to patch_tf_keras\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\wandb\\integration\\keras\\keras.py\", line 86, in patch_tf_keras\n",
      "    from keras.engine import training\n",
      "ModuleNotFoundError: No module named 'keras.engine'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the random generator seeds for better reproducibility\n",
    "tf.random.set_seed(67)\n",
    "np.random.seed(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Class to build the Model\n",
    "class Model_builder:\n",
    "        def __init__(self, input_shape, num_classes):\n",
    "            self.input_shape = input_shape\n",
    "            self.num_classes = num_classes\n",
    "            self.model = None\n",
    "        def build(self):\n",
    "\n",
    "        \n",
    "\n",
    "            input_img = Input(shape=self.input_shape)\n",
    "            x= input_img\n",
    "            # AUTOENCODER: how stacked should it be/How many layers? \n",
    "            #1st layer\n",
    "            for _ in range(5):\n",
    "                x = Conv2D(\n",
    "                    filters=32,\n",
    "                    kernel_size=3, #could be changed manually\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    activity_regularizer=regularizers.l1(0.0001),\n",
    "                    kernel_initializer='he_normal'\n",
    "                )(x)\n",
    "                x = MaxPooling2D((2, 2), padding='same')(x) #max pooling layer\n",
    "\n",
    "            #max pooling layer to give us the result of the encoding process: latent space\n",
    "            encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "            flattened = tf.keras.layers.Flatten()(encoded)\n",
    "            dense_1 = tf.keras.layers.Dense(1344, activation='relu')(flattened)\n",
    "            dense_2 = tf.keras.layers.Dense(100, activation='relu')(dense_1)\n",
    "            output = tf.keras.layers.Dense(2, activation='softmax')(dense_2)\n",
    "                   \n",
    "                        \n",
    "                        \n",
    "                        #start of decoding, remove comment for decoding\n",
    "                        #x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "                        #x = UpSampling2D((2, 2))(x)\n",
    "                        #x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "                        #x = UpSampling2D((2, 2))(x)\n",
    "                        #decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "            #PRETRAINED EFFICIENTNET, could possibly use smaller but less computationaly expensive models: try,\n",
    "            # note that use of LSTM is computationally more expensive and is not sure to bring better accuracy but still worth trying \n",
    "            #also note the use of tranfer learning here, but could also go with the route of training the whole model: after HP tuning with validation data ofc\n",
    "            #efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "            #for layer in efficientnet.layers[:-20]:  #freezing everything except top 20 layers, again could also go for layer.trainable=true for all layers: do we have enough computational resources?\n",
    "                #layer.trainable = False\n",
    "            #x = efficientnet(encoded) #!!!!!!!!!!!!!!!!!!!!!!!change to encoded, change dimensions in other places accordingly\n",
    "            #x = GlobalAveragePooling2D()(x) #helps reduce overfitting by reducing the total number of parameters in the model\n",
    "            #x = Dense(32,kernel_initializer='glorot_uniform')(x) #A fully connected dense layer, possibly for feature extracion, could think of adding more layers here for feature classifications in the future\n",
    "            #x = Dropout(0.3)(x)#for regularization, again use of tuning: maybe too much?\n",
    "            #output = Dense(self.num_classes, activation='softmax')(x)#fully connected dense layer with softmax activation for producing the output probabilities of classes\n",
    "                #This works as the classifier in our ML pipeline thanks to softmax\n",
    "\n",
    "\n",
    "            #create the model using keras Model function with all the earlier configuration and return it\n",
    "            self.model = tf.keras.models.Model(inputs=input_img, outputs=output)\n",
    "\n",
    "        \n",
    "        #compile the model\n",
    "        def compile(self):\n",
    "            self.model.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            return self.model\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a model_builder object, build and compile the model inside it and retrieve it as \"model\"\n",
    "model_builder_object = Model_builder(input_shape=(240, 193, 1) ,num_classes=2)#this part can be replaced with correct input shape: 240x193x1\n",
    "model_builder_object.build() \n",
    "model = model_builder_object.compile()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 80 files for training.\n",
      "Found 100 files belonging to 2 classes.\n",
      "Using 20 files for validation.\n"
     ]
    }
   ],
   "source": [
    "directory = r\"C:\\Users\\bbaki\\Desktop\\fhnw\\Scripts\\radio_sunburst_detector\\data\"\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    class_names=None,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(240, 193),\n",
    "    shuffle=True,\n",
    "    seed=42, #can change\n",
    "    validation_split=0.2, #can change\n",
    "    subset=\"training\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    class_names=None,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(240, 193),\n",
    "    shuffle=True,\n",
    "    seed=42, #can change\n",
    "    validation_split=0.2, #can change\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale training and validation data\n",
    "def rescale(image, label):\n",
    "    return image/255. , label\n",
    "\n",
    "train_ds = train_ds.map(rescale)\n",
    "validation_ds = validation_ds.map(rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Image number:  3\n",
      "Validation data Image number:  1\n"
     ]
    }
   ],
   "source": [
    "#Determine the number of images in train and validation datasets\n",
    "train_total_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "val_total_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
    "\n",
    "print(\"Training data Image number: \", train_total_size)\n",
    "print(\"Validation data Image number: \", val_total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the data for the train test split to \n",
    "#dataset = dataset.shuffle(total_size)\n",
    "\n",
    "#Split into train and test sets\n",
    "#train_dataset = dataset.take(train_size)\n",
    "#test_dataset = dataset.skip(train_size)\n",
    "\n",
    "#Function to split dataset into inputs (x) and labels (y)\n",
    "#def split_into_inputs_and_labels(image, label):\n",
    "#    return image, label\n",
    "\n",
    "#Apply this function to both datasets using map function: Iteratively apply the split_into_inputs_and_labels element to \n",
    "#every element in train_dataset and test_dataset which consist of image and label thanks to keras image_dataset_from_directory function\n",
    "#train_x, train_y = zip(*train_dataset.map(split_into_inputs_and_labels))\n",
    "#test_x, test_y = zip(*test_dataset.map(split_into_inputs_and_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_3\" is incompatible with the layer: expected shape=(None, 8, 7, 32), found shape=(None, 240, 193, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m     train_ds,\n\u001b[0;32m      4\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(validation_ds),\n\u001b[0;32m      5\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej75s9ktg.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_3\" is incompatible with the layer: expected shape=(None, 8, 7, 32), found shape=(None, 240, 193, 1)\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=(validation_ds),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
