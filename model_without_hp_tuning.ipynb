{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the random generator seeds for better reproducibility\n",
    "tf.random.set_seed(67)\n",
    "np.random.seed(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Class to build the Model\n",
    "class Model_builder:\n",
    "        def __init__(self, input_shape, num_classes):\n",
    "            self.input_shape = input_shape\n",
    "            self.num_classes = num_classes\n",
    "            self.model = None\n",
    "        def build(self):\n",
    "\n",
    "        \n",
    "\n",
    "            input_img = Input(shape=self.input_shape)\n",
    "            x= input_img\n",
    "            # AUTOENCODER: how stacked should it be/How many layers? \n",
    "            #1st layer\n",
    "            for _ in range(5):\n",
    "                x = Conv2D(\n",
    "                    filters=32,\n",
    "                    kernel_size=3, #could be changed manually\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    activity_regularizer=regularizers.l1(0.0001),\n",
    "                    kernel_initializer='he_normal'\n",
    "                )(x)\n",
    "                x = MaxPooling2D((2, 2), padding='same')(x) #max pooling layer\n",
    "\n",
    "            #max pooling layer to give us the result of the encoding process: latent space\n",
    "            encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "            print(encoded.shape)\n",
    "\n",
    "            flattened = tf.keras.layers.Flatten()(encoded)\n",
    "            dense_1 = tf.keras.layers.Dense(1792, activation='relu')(flattened)\n",
    "            dense_2 = tf.keras.layers.Dense(100, activation='relu')(dense_1)\n",
    "            output = tf.keras.layers.Dense(2, activation='softmax')(dense_2)\n",
    "                   \n",
    "                        \n",
    "                        \n",
    "                        #start of decoding, remove comment for decoding\n",
    "                        #x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "                        #x = UpSampling2D((2, 2))(x)\n",
    "                        #x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "                        #x = UpSampling2D((2, 2))(x)\n",
    "                        #decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "            #PRETRAINED EFFICIENTNET, could possibly use smaller but less computationaly expensive models: try,\n",
    "            # note that use of LSTM is computationally more expensive and is not sure to bring better accuracy but still worth trying \n",
    "            #also note the use of tranfer learning here, but could also go with the route of training the whole model: after HP tuning with validation data ofc\n",
    "            #efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "            #for layer in efficientnet.layers[:-20]:  #freezing everything except top 20 layers, again could also go for layer.trainable=true for all layers: do we have enough computational resources?\n",
    "                #layer.trainable = False\n",
    "            #x = efficientnet(encoded) #!!!!!!!!!!!!!!!!!!!!!!!change to encoded, change dimensions in other places accordingly\n",
    "            #x = GlobalAveragePooling2D()(x) #helps reduce overfitting by reducing the total number of parameters in the model\n",
    "            #x = Dense(32,kernel_initializer='glorot_uniform')(x) #A fully connected dense layer, possibly for feature extracion, could think of adding more layers here for feature classifications in the future\n",
    "            #x = Dropout(0.3)(x)#for regularization, again use of tuning: maybe too much?\n",
    "            #output = Dense(self.num_classes, activation='softmax')(x)#fully connected dense layer with softmax activation for producing the output probabilities of classes\n",
    "                #This works as the classifier in our ML pipeline thanks to softmax\n",
    "\n",
    "\n",
    "            #create the model using keras Model function with all the earlier configuration and return it\n",
    "            self.model = tf.keras.models.Model(inputs=input_img, outputs=output)\n",
    "\n",
    "        \n",
    "        #compile the model\n",
    "        def compile(self):\n",
    "            self.model.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy', \n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            return self.model\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 7, 32)\n"
     ]
    }
   ],
   "source": [
    "#Create a model_builder object, build and compile the model inside it and retrieve it as \"model\"\n",
    "model_builder_object = Model_builder(input_shape=(240, 193, 1) ,num_classes=2)#this part can be replaced with correct input shape: 240x193x1\n",
    "model_builder_object.build() \n",
    "model = model_builder_object.compile()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 80 files for training.\n",
      "Found 100 files belonging to 2 classes.\n",
      "Using 20 files for validation.\n"
     ]
    }
   ],
   "source": [
    "directory = r\"C:\\Users\\bbaki\\Desktop\\fhnw\\Scripts\\radio_sunburst_detector\\data\"\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    class_names=None,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(240, 193),\n",
    "    shuffle=True,\n",
    "    seed=42, #can change\n",
    "    validation_split=0.2, #can change\n",
    "    subset=\"training\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    class_names=None,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(240, 193),\n",
    "    shuffle=True,\n",
    "    seed=42, #can change\n",
    "    validation_split=0.2, #can change\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale training and validation data\n",
    "def rescale(image, label):\n",
    "    return image/255. , label\n",
    "\n",
    "train_ds = train_ds.map(rescale)\n",
    "validation_ds = validation_ds.map(rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Image number:  3\n",
      "Validation data Image number:  1\n"
     ]
    }
   ],
   "source": [
    "#Determine the number of images in train and validation datasets\n",
    "train_total_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "val_total_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
    "\n",
    "print(\"Training data Image number: \", train_total_size)\n",
    "print(\"Validation data Image number: \", val_total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the data for the train test split to \n",
    "#dataset = dataset.shuffle(total_size)\n",
    "\n",
    "#Split into train and test sets\n",
    "#train_dataset = dataset.take(train_size)\n",
    "#test_dataset = dataset.skip(train_size)\n",
    "\n",
    "#Function to split dataset into inputs (x) and labels (y)\n",
    "#def split_into_inputs_and_labels(image, label):\n",
    "#    return image, label\n",
    "\n",
    "#Apply this function to both datasets using map function: Iteratively apply the split_into_inputs_and_labels element to \n",
    "#every element in train_dataset and test_dataset which consist of image and label thanks to keras image_dataset_from_directory function\n",
    "#train_x, train_y = zip(*train_dataset.map(split_into_inputs_and_labels))\n",
    "#test_x, test_y = zip(*test_dataset.map(split_into_inputs_and_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 3s 911ms/step - loss: 20.1197 - accuracy: 0.5500 - val_loss: 21.7307 - val_accuracy: 0.4000\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 3s 880ms/step - loss: 19.1236 - accuracy: 0.5250 - val_loss: 20.7402 - val_accuracy: 0.4000\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 3s 921ms/step - loss: 18.2408 - accuracy: 0.5250 - val_loss: 19.8446 - val_accuracy: 0.4000\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 3s 971ms/step - loss: 17.4617 - accuracy: 0.5250 - val_loss: 19.0682 - val_accuracy: 0.4000\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 3s 932ms/step - loss: 16.7767 - accuracy: 0.5250 - val_loss: 18.3693 - val_accuracy: 0.4000\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 3s 924ms/step - loss: 16.1527 - accuracy: 0.5375 - val_loss: 17.7252 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 3s 926ms/step - loss: 15.5830 - accuracy: 0.5375 - val_loss: 17.1309 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 3s 861ms/step - loss: 15.0552 - accuracy: 0.5500 - val_loss: 16.5731 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 3s 850ms/step - loss: 14.5620 - accuracy: 0.5375 - val_loss: 16.0421 - val_accuracy: 0.6000\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 3s 865ms/step - loss: 14.0837 - accuracy: 0.6250 - val_loss: 15.5455 - val_accuracy: 0.5500\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 3s 1s/step - loss: 13.6340 - accuracy: 0.5625 - val_loss: 15.0648 - val_accuracy: 0.6000\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 3s 866ms/step - loss: 13.1817 - accuracy: 0.5875 - val_loss: 14.5773 - val_accuracy: 0.7000\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 3s 850ms/step - loss: 12.7613 - accuracy: 0.6500 - val_loss: 14.1065 - val_accuracy: 0.6000\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 3s 855ms/step - loss: 12.3389 - accuracy: 0.6375 - val_loss: 13.6654 - val_accuracy: 0.7000\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 3s 839ms/step - loss: 11.9457 - accuracy: 0.6125 - val_loss: 13.2577 - val_accuracy: 0.6000\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 3s 793ms/step - loss: 11.5307 - accuracy: 0.6375 - val_loss: 12.7807 - val_accuracy: 0.7500\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 3s 892ms/step - loss: 11.1283 - accuracy: 0.6625 - val_loss: 12.3381 - val_accuracy: 0.7000\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 3s 812ms/step - loss: 10.7383 - accuracy: 0.7125 - val_loss: 11.9152 - val_accuracy: 0.7000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 3s 831ms/step - loss: 10.3485 - accuracy: 0.6875 - val_loss: 11.5132 - val_accuracy: 0.6500\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 3s 826ms/step - loss: 9.9642 - accuracy: 0.6625 - val_loss: 11.1263 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x286fc608a10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=(validation_ds),\n",
    "    epochs=20,\n",
    "    batch_size=4,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
