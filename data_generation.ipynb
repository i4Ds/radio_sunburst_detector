{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from ecallisto_ng.data_fetching.get_data import extract_instrument_name, get_data\n",
    "from ecallisto_ng.data_fetching.get_information import get_tables, get_table_names_with_data_between_dates, check_table_data_availability\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation Radio Sunburst Detector\n",
    "## Create images with bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_list = pd.read_excel('burst_list.xlsx').dropna(subset=['instruments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_list.loc[:, 'instruments'] = burst_list.instruments.apply(extract_instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>instruments</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_end</th>\n",
       "      <th>date_start</th>\n",
       "      <th>date_end</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210119</td>\n",
       "      <td>02:42-02:42</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>02:42</td>\n",
       "      <td>02:42</td>\n",
       "      <td>20210119</td>\n",
       "      <td>20210119</td>\n",
       "      <td>2021-01-19 02:42:00</td>\n",
       "      <td>2021-01-19 02:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210120</td>\n",
       "      <td>12:37-12:37</td>\n",
       "      <td>3</td>\n",
       "      <td>austria_unigraz</td>\n",
       "      <td>12:37</td>\n",
       "      <td>12:37</td>\n",
       "      <td>20210120</td>\n",
       "      <td>20210120</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210120</td>\n",
       "      <td>12:37-12:37</td>\n",
       "      <td>3</td>\n",
       "      <td>humain</td>\n",
       "      <td>12:37</td>\n",
       "      <td>12:37</td>\n",
       "      <td>20210120</td>\n",
       "      <td>20210120</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210120</td>\n",
       "      <td>12:37-12:37</td>\n",
       "      <td>3</td>\n",
       "      <td>mrt1</td>\n",
       "      <td>12:37</td>\n",
       "      <td>12:37</td>\n",
       "      <td>20210120</td>\n",
       "      <td>20210120</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210120</td>\n",
       "      <td>12:37-12:37</td>\n",
       "      <td>3</td>\n",
       "      <td>southafrica_sansa</td>\n",
       "      <td>12:37</td>\n",
       "      <td>12:37</td>\n",
       "      <td>20210120</td>\n",
       "      <td>20210120</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "      <td>2021-01-20 12:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32464</th>\n",
       "      <td>20230707</td>\n",
       "      <td>10:15-10:16</td>\n",
       "      <td>4</td>\n",
       "      <td>germany_dlr</td>\n",
       "      <td>10:15</td>\n",
       "      <td>10:16</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 10:15:00</td>\n",
       "      <td>2023-07-07 10:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32465</th>\n",
       "      <td>20230707</td>\n",
       "      <td>10:15-10:16</td>\n",
       "      <td>4</td>\n",
       "      <td>norway_egersund</td>\n",
       "      <td>10:15</td>\n",
       "      <td>10:16</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 10:15:00</td>\n",
       "      <td>2023-07-07 10:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32466</th>\n",
       "      <td>20230707</td>\n",
       "      <td>10:15-10:16</td>\n",
       "      <td>4</td>\n",
       "      <td>swiss_heiterswil</td>\n",
       "      <td>10:15</td>\n",
       "      <td>10:16</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 10:15:00</td>\n",
       "      <td>2023-07-07 10:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32467</th>\n",
       "      <td>20230707</td>\n",
       "      <td>10:15-10:16</td>\n",
       "      <td>4</td>\n",
       "      <td>swiss_landschlacht</td>\n",
       "      <td>10:15</td>\n",
       "      <td>10:16</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 10:15:00</td>\n",
       "      <td>2023-07-07 10:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32468</th>\n",
       "      <td>20230707</td>\n",
       "      <td>10:42-10:43</td>\n",
       "      <td>3</td>\n",
       "      <td>germany_dlr</td>\n",
       "      <td>10:42</td>\n",
       "      <td>10:43</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 10:42:00</td>\n",
       "      <td>2023-07-07 10:43:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32462 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         time  type         instruments time_start time_end  \\\n",
       "0      20210119  02:42-02:42     3      australia_assa      02:42    02:42   \n",
       "1      20210120  12:37-12:37     3     austria_unigraz      12:37    12:37   \n",
       "2      20210120  12:37-12:37     3              humain      12:37    12:37   \n",
       "3      20210120  12:37-12:37     3                mrt1      12:37    12:37   \n",
       "4      20210120  12:37-12:37     3   southafrica_sansa      12:37    12:37   \n",
       "...         ...          ...   ...                 ...        ...      ...   \n",
       "32464  20230707  10:15-10:16     4         germany_dlr      10:15    10:16   \n",
       "32465  20230707  10:15-10:16     4     norway_egersund      10:15    10:16   \n",
       "32466  20230707  10:15-10:16     4    swiss_heiterswil      10:15    10:16   \n",
       "32467  20230707  10:15-10:16     4  swiss_landschlacht      10:15    10:16   \n",
       "32468  20230707  10:42-10:43     3         germany_dlr      10:42    10:43   \n",
       "\n",
       "       date_start  date_end      datetime_start        datetime_end  \n",
       "0        20210119  20210119 2021-01-19 02:42:00 2021-01-19 02:42:00  \n",
       "1        20210120  20210120 2021-01-20 12:37:00 2021-01-20 12:37:00  \n",
       "2        20210120  20210120 2021-01-20 12:37:00 2021-01-20 12:37:00  \n",
       "3        20210120  20210120 2021-01-20 12:37:00 2021-01-20 12:37:00  \n",
       "4        20210120  20210120 2021-01-20 12:37:00 2021-01-20 12:37:00  \n",
       "...           ...       ...                 ...                 ...  \n",
       "32464    20230707  20230707 2023-07-07 10:15:00 2023-07-07 10:16:00  \n",
       "32465    20230707  20230707 2023-07-07 10:15:00 2023-07-07 10:16:00  \n",
       "32466    20230707  20230707 2023-07-07 10:15:00 2023-07-07 10:16:00  \n",
       "32467    20230707  20230707 2023-07-07 10:15:00 2023-07-07 10:16:00  \n",
       "32468    20230707  20230707 2023-07-07 10:42:00 2023-07-07 10:43:00  \n",
       "\n",
       "[32462 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burst_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['australia_assa_02',\n",
       " 'australia_assa_63',\n",
       " 'australia_assa_01',\n",
       " 'australia_assa_56',\n",
       " 'australia_lmro_59',\n",
       " 'australia_assa_57',\n",
       " 'australia_assa_62',\n",
       " 'australia_assa_60']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in get_tables() if 'australia' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>instruments</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_end</th>\n",
       "      <th>date_start</th>\n",
       "      <th>date_end</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210119</td>\n",
       "      <td>02:42-02:42</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>02:42</td>\n",
       "      <td>02:42</td>\n",
       "      <td>20210119</td>\n",
       "      <td>20210119</td>\n",
       "      <td>2021-01-19 02:42:00</td>\n",
       "      <td>2021-01-19 02:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20210127</td>\n",
       "      <td>04:32-04:32</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>04:32</td>\n",
       "      <td>04:32</td>\n",
       "      <td>20210127</td>\n",
       "      <td>20210127</td>\n",
       "      <td>2021-01-27 04:32:00</td>\n",
       "      <td>2021-01-27 04:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>20210419</td>\n",
       "      <td>06:55-06:57</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>06:55</td>\n",
       "      <td>06:57</td>\n",
       "      <td>20210419</td>\n",
       "      <td>20210419</td>\n",
       "      <td>2021-04-19 06:55:00</td>\n",
       "      <td>2021-04-19 06:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>20210419</td>\n",
       "      <td>23:39-23:42</td>\n",
       "      <td>2</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>23:39</td>\n",
       "      <td>23:42</td>\n",
       "      <td>20210419</td>\n",
       "      <td>20210419</td>\n",
       "      <td>2021-04-19 23:39:00</td>\n",
       "      <td>2021-04-19 23:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>20210419</td>\n",
       "      <td>23:39-23:43</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>23:39</td>\n",
       "      <td>23:43</td>\n",
       "      <td>20210419</td>\n",
       "      <td>20210419</td>\n",
       "      <td>2021-04-19 23:39:00</td>\n",
       "      <td>2021-04-19 23:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32421</th>\n",
       "      <td>20230707</td>\n",
       "      <td>00:53-00:54</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>00:53</td>\n",
       "      <td>00:54</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 00:53:00</td>\n",
       "      <td>2023-07-07 00:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32424</th>\n",
       "      <td>20230707</td>\n",
       "      <td>01:33-01:34</td>\n",
       "      <td>5</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>01:33</td>\n",
       "      <td>01:34</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 01:33:00</td>\n",
       "      <td>2023-07-07 01:34:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32427</th>\n",
       "      <td>20230707</td>\n",
       "      <td>01:36-01:36</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>01:36</td>\n",
       "      <td>01:36</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 01:36:00</td>\n",
       "      <td>2023-07-07 01:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32431</th>\n",
       "      <td>20230707</td>\n",
       "      <td>04:43-04:46</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>04:43</td>\n",
       "      <td>04:46</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 04:43:00</td>\n",
       "      <td>2023-07-07 04:46:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32440</th>\n",
       "      <td>20230707</td>\n",
       "      <td>05:29-05:29</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>05:29</td>\n",
       "      <td>05:29</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 05:29:00</td>\n",
       "      <td>2023-07-07 05:29:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2490 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         time  type     instruments time_start time_end  \\\n",
       "0      20210119  02:42-02:42     3  australia_assa      02:42    02:42   \n",
       "7      20210127  04:32-04:32     3  australia_assa      04:32    04:32   \n",
       "89     20210419  06:55-06:57     3  australia_assa      06:55    06:57   \n",
       "108    20210419  23:39-23:42     2  australia_assa      23:39    23:42   \n",
       "109    20210419  23:39-23:43     3  australia_assa      23:39    23:43   \n",
       "...         ...          ...   ...             ...        ...      ...   \n",
       "32421  20230707  00:53-00:54     3  australia_assa      00:53    00:54   \n",
       "32424  20230707  01:33-01:34     5  australia_assa      01:33    01:34   \n",
       "32427  20230707  01:36-01:36     3  australia_assa      01:36    01:36   \n",
       "32431  20230707  04:43-04:46     3  australia_assa      04:43    04:46   \n",
       "32440  20230707  05:29-05:29     3  australia_assa      05:29    05:29   \n",
       "\n",
       "       date_start  date_end      datetime_start        datetime_end  \n",
       "0        20210119  20210119 2021-01-19 02:42:00 2021-01-19 02:42:00  \n",
       "7        20210127  20210127 2021-01-27 04:32:00 2021-01-27 04:32:00  \n",
       "89       20210419  20210419 2021-04-19 06:55:00 2021-04-19 06:57:00  \n",
       "108      20210419  20210419 2021-04-19 23:39:00 2021-04-19 23:42:00  \n",
       "109      20210419  20210419 2021-04-19 23:39:00 2021-04-19 23:43:00  \n",
       "...           ...       ...                 ...                 ...  \n",
       "32421    20230707  20230707 2023-07-07 00:53:00 2023-07-07 00:54:00  \n",
       "32424    20230707  20230707 2023-07-07 01:33:00 2023-07-07 01:34:00  \n",
       "32427    20230707  20230707 2023-07-07 01:36:00 2023-07-07 01:36:00  \n",
       "32431    20230707  20230707 2023-07-07 04:43:00 2023-07-07 04:46:00  \n",
       "32440    20230707  20230707 2023-07-07 05:29:00 2023-07-07 05:29:00  \n",
       "\n",
       "[2490 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PARAMETERS ###\n",
    "IMAGE_LENGTH = timedelta(minutes=1)\n",
    "PIXEL_PER_IMAGE_OVER_TIME = 200\n",
    "PIXEL_PER_IMAGE_OVER_FREQUENCY = 200\n",
    "INSTRUMENTS_TO_INCLUDE = ['australia_assa_01', ]\n",
    "TOTAL_IMAGE_NUM = 1000\n",
    "###\n",
    "time_bucket = IMAGE_LENGTH.total_seconds\n",
    "\n",
    "# Filter burst list\n",
    "def remove_id_from_instrument_name(instrument_name):\n",
    "    return '_'.join(instrument_name.split('_')[:-1])\n",
    "\n",
    "instruments_to_include_sql_table_compatible = [remove_id_from_instrument_name(instrument) for instrument in INSTRUMENTS_TO_INCLUDE]\n",
    "# Drop duplicate list\n",
    "instruments_to_include_sql_table_compatible = list(set(instruments_to_include_sql_table_compatible))\n",
    "burst_list_filtered = burst_list[burst_list.instruments.isin(instruments_to_include_sql_table_compatible)]\n",
    "burst_list_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find good instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change it to true if you want to create some images of selected instrument (to e.g. select only nice instruments, manually.)\n",
    "if False:\n",
    "    n = 3\n",
    "    burst_list_filtered = burst_list_filtered.groupby('instruments').sample(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>instruments</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_end</th>\n",
       "      <th>date_start</th>\n",
       "      <th>date_end</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210119</td>\n",
       "      <td>02:42-02:42</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>02:42</td>\n",
       "      <td>02:42</td>\n",
       "      <td>20210119</td>\n",
       "      <td>20210119</td>\n",
       "      <td>2021-01-19 02:42:00</td>\n",
       "      <td>2021-01-19 02:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20210127</td>\n",
       "      <td>04:32-04:32</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>04:32</td>\n",
       "      <td>04:32</td>\n",
       "      <td>20210127</td>\n",
       "      <td>20210127</td>\n",
       "      <td>2021-01-27 04:32:00</td>\n",
       "      <td>2021-01-27 04:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>20210419</td>\n",
       "      <td>06:55-06:57</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>06:55</td>\n",
       "      <td>06:57</td>\n",
       "      <td>20210419</td>\n",
       "      <td>20210419</td>\n",
       "      <td>2021-04-19 06:55:00</td>\n",
       "      <td>2021-04-19 06:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>20210419</td>\n",
       "      <td>23:39-23:42</td>\n",
       "      <td>2</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>23:39</td>\n",
       "      <td>23:42</td>\n",
       "      <td>20210419</td>\n",
       "      <td>20210419</td>\n",
       "      <td>2021-04-19 23:39:00</td>\n",
       "      <td>2021-04-19 23:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>20210419</td>\n",
       "      <td>23:39-23:43</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>23:39</td>\n",
       "      <td>23:43</td>\n",
       "      <td>20210419</td>\n",
       "      <td>20210419</td>\n",
       "      <td>2021-04-19 23:39:00</td>\n",
       "      <td>2021-04-19 23:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32421</th>\n",
       "      <td>20230707</td>\n",
       "      <td>00:53-00:54</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>00:53</td>\n",
       "      <td>00:54</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 00:53:00</td>\n",
       "      <td>2023-07-07 00:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32424</th>\n",
       "      <td>20230707</td>\n",
       "      <td>01:33-01:34</td>\n",
       "      <td>5</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>01:33</td>\n",
       "      <td>01:34</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 01:33:00</td>\n",
       "      <td>2023-07-07 01:34:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32427</th>\n",
       "      <td>20230707</td>\n",
       "      <td>01:36-01:36</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>01:36</td>\n",
       "      <td>01:36</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 01:36:00</td>\n",
       "      <td>2023-07-07 01:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32431</th>\n",
       "      <td>20230707</td>\n",
       "      <td>04:43-04:46</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>04:43</td>\n",
       "      <td>04:46</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 04:43:00</td>\n",
       "      <td>2023-07-07 04:46:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32440</th>\n",
       "      <td>20230707</td>\n",
       "      <td>05:29-05:29</td>\n",
       "      <td>3</td>\n",
       "      <td>australia_assa</td>\n",
       "      <td>05:29</td>\n",
       "      <td>05:29</td>\n",
       "      <td>20230707</td>\n",
       "      <td>20230707</td>\n",
       "      <td>2023-07-07 05:29:00</td>\n",
       "      <td>2023-07-07 05:29:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2490 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         time  type     instruments time_start time_end  \\\n",
       "0      20210119  02:42-02:42     3  australia_assa      02:42    02:42   \n",
       "7      20210127  04:32-04:32     3  australia_assa      04:32    04:32   \n",
       "89     20210419  06:55-06:57     3  australia_assa      06:55    06:57   \n",
       "108    20210419  23:39-23:42     2  australia_assa      23:39    23:42   \n",
       "109    20210419  23:39-23:43     3  australia_assa      23:39    23:43   \n",
       "...         ...          ...   ...             ...        ...      ...   \n",
       "32421  20230707  00:53-00:54     3  australia_assa      00:53    00:54   \n",
       "32424  20230707  01:33-01:34     5  australia_assa      01:33    01:34   \n",
       "32427  20230707  01:36-01:36     3  australia_assa      01:36    01:36   \n",
       "32431  20230707  04:43-04:46     3  australia_assa      04:43    04:46   \n",
       "32440  20230707  05:29-05:29     3  australia_assa      05:29    05:29   \n",
       "\n",
       "       date_start  date_end      datetime_start        datetime_end  \n",
       "0        20210119  20210119 2021-01-19 02:42:00 2021-01-19 02:42:00  \n",
       "7        20210127  20210127 2021-01-27 04:32:00 2021-01-27 04:32:00  \n",
       "89       20210419  20210419 2021-04-19 06:55:00 2021-04-19 06:57:00  \n",
       "108      20210419  20210419 2021-04-19 23:39:00 2021-04-19 23:42:00  \n",
       "109      20210419  20210419 2021-04-19 23:39:00 2021-04-19 23:43:00  \n",
       "...           ...       ...                 ...                 ...  \n",
       "32421    20230707  20230707 2023-07-07 00:53:00 2023-07-07 00:54:00  \n",
       "32424    20230707  20230707 2023-07-07 01:33:00 2023-07-07 01:34:00  \n",
       "32427    20230707  20230707 2023-07-07 01:36:00 2023-07-07 01:36:00  \n",
       "32431    20230707  20230707 2023-07-07 04:43:00 2023-07-07 04:46:00  \n",
       "32440    20230707  20230707 2023-07-07 05:29:00 2023-07-07 05:29:00  \n",
       "\n",
       "[2490 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burst_list_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create images of bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_save_as_img(instrument, start_datetime, end_datetime, time_bucket, agg_function='MAX', burst_type=\"no_burst\", min_shape=(200, 200), data_folder=\"data\"):\n",
    "    \"\"\"\n",
    "    Retrieves data for a specific instrument within a given time range, aggregates it using the specified function,\n",
    "    normalizes the data, and saves it as an image file.\n",
    "\n",
    "    Args:\n",
    "        instrument (str): Name of the instrument for which data is to be retrieved.\n",
    "        start_datetime (datetime.datetime): Start date and time of the data range.\n",
    "        end_datetime (datetime.datetime): End date and time of the data range.\n",
    "        time_bucket (str): Time granularity for data aggregation (e.g., '1H' for hourly, '30T' for every 30 minutes).\n",
    "        agg_function (str, optional): Aggregation function to apply to the data. Defaults to 'MAX'.\n",
    "        burst_type (str, optional): Label to be included in the file name. Defaults to 'no_burst'.\n",
    "        data_folder (str, optional): Folder path where the data will be saved. Defaults to 'data'.\n",
    "        min_shape (tuple, optional): Minimum shape of the image. Defaults to (200, 200).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Examples:\n",
    "        # Retrieve data for instrument 'instrument_name' from 'start_datetime' to 'end_datetime' and save it as an image\n",
    "        get_data_save_as_img('instrument_name', start_datetime, end_datetime, '1H', 'MAX', 'no_burst', 'data')\n",
    "\n",
    "    \"\"\"\n",
    "    sd_str = start_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    ed_str = end_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df = get_data(\n",
    "        instrument_name=instrument, \n",
    "        start_datetime=sd_str,\n",
    "        end_datetime=ed_str, \n",
    "        timebucket=time_bucket, \n",
    "        agg_function=agg_function\n",
    "        )\n",
    "    \n",
    "    img_data = df.to_numpy().astype(np.int16)\n",
    "    if not img_data.shape[0] >= min_shape[0] and img_data.shape[1] >= min_shape[1]: \n",
    "        raise ValueError(\"Image shape is too small.\")\n",
    "    # Generate path\n",
    "    path = os.path.join(data_folder, burst_type)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_path = os.path.join(path, sd_str.replace(':', '-') + \"_\" + ed_str.replace(':', '-') + \"_\" + instrument + \"_\" + str(time_bucket) + \".png\")\n",
    "    plt.imsave(file_path, img_data.T, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping australia_assa_01 from 2022-03-30 05:50:00 to 2022-03-30 05:51:00\n",
      "Skipping australia_assa_01 from 2022-03-30 05:51:00 to 2022-03-30 05:52:00\n",
      "Skipping australia_assa_01 from 2022-03-30 05:52:00 to 2022-03-30 05:53:00\n",
      "Skipping australia_assa_01 from 2021-12-17 05:42:00 to 2021-12-17 05:43:00\n",
      "0   australia_assa_01  -----  2021-09-26 23:46:00  to  2021-09-26 23:47:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2022-04-20 03:55:00 to 2022-04-20 03:56:00\n",
      "Skipping australia_assa_01 from 2022-04-20 03:56:00 to 2022-04-20 03:57:00\n",
      "Skipping australia_assa_01 from 2022-04-20 03:57:00 to 2022-04-20 03:58:00\n",
      "Skipping australia_assa_01 from 2022-04-20 03:58:00 to 2022-04-20 03:59:00\n",
      "Skipping australia_assa_01 from 2022-04-20 03:59:00 to 2022-04-20 04:00:00\n",
      "Skipping australia_assa_01 from 2022-04-20 04:00:00 to 2022-04-20 04:01:00\n",
      "Skipping australia_assa_01 from 2022-04-20 04:01:00 to 2022-04-20 04:02:00\n",
      "Skipping australia_assa_01 from 2022-04-20 04:02:00 to 2022-04-20 04:03:00\n",
      "Skipping australia_assa_01 from 2022-09-30 01:40:00 to 2022-09-30 01:41:00\n",
      "1   australia_assa_01  -----  2021-09-25 03:45:00  to  2021-09-25 03:46:00\n",
      "File downloaded successfully\n",
      "2   australia_assa_01  -----  2021-09-25 03:46:00  to  2021-09-25 03:47:00\n",
      "File downloaded successfully\n",
      "3   australia_assa_01  -----  2021-09-25 03:47:00  to  2021-09-25 03:48:00\n",
      "File downloaded successfully\n",
      "4   australia_assa_01  -----  2021-09-25 03:48:00  to  2021-09-25 03:49:00\n",
      "File downloaded successfully\n",
      "5   australia_assa_01  -----  2021-09-25 03:49:00  to  2021-09-25 03:50:00\n",
      "File downloaded successfully\n",
      "6   australia_assa_01  -----  2021-09-25 03:50:00  to  2021-09-25 03:51:00\n",
      "File downloaded successfully\n",
      "7   australia_assa_01  -----  2021-09-25 03:51:00  to  2021-09-25 03:52:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2023-01-27 04:08:00 to 2023-01-27 04:09:00\n",
      "Skipping australia_assa_01 from 2023-01-27 04:09:00 to 2023-01-27 04:10:00\n",
      "Skipping australia_assa_01 from 2023-01-27 04:10:00 to 2023-01-27 04:11:00\n",
      "Skipping australia_assa_01 from 2023-01-27 04:11:00 to 2023-01-27 04:12:00\n",
      "Skipping australia_assa_01 from 2023-01-27 04:12:00 to 2023-01-27 04:13:00\n",
      "Skipping australia_assa_01 from 2023-01-27 04:13:00 to 2023-01-27 04:14:00\n",
      "Skipping australia_assa_01 from 2023-02-09 22:15:00 to 2023-02-09 22:16:00\n",
      "Skipping australia_assa_01 from 2022-09-14 01:58:00 to 2022-09-14 01:59:00\n",
      "Skipping australia_assa_01 from 2022-09-14 01:59:00 to 2022-09-14 02:00:00\n",
      "Skipping australia_assa_01 from 2022-09-14 02:00:00 to 2022-09-14 02:01:00\n",
      "Skipping australia_assa_01 from 2022-09-14 02:01:00 to 2022-09-14 02:02:00\n",
      "Skipping australia_assa_01 from 2022-09-14 02:02:00 to 2022-09-14 02:03:00\n",
      "Skipping australia_assa_01 from 2022-09-14 02:03:00 to 2022-09-14 02:04:00\n",
      "Skipping australia_assa_01 from 2022-09-14 02:04:00 to 2022-09-14 02:05:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:22:00 to 2022-03-25 05:23:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:23:00 to 2022-03-25 05:24:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:24:00 to 2022-03-25 05:25:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:25:00 to 2022-03-25 05:26:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:26:00 to 2022-03-25 05:27:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:27:00 to 2022-03-25 05:28:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:28:00 to 2022-03-25 05:29:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:29:00 to 2022-03-25 05:30:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:30:00 to 2022-03-25 05:31:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:31:00 to 2022-03-25 05:32:00\n",
      "Skipping australia_assa_01 from 2022-03-25 05:32:00 to 2022-03-25 05:33:00\n",
      "8   australia_assa_01  -----  2021-05-22 03:26:00  to  2021-05-22 03:27:00\n",
      "File downloaded successfully\n",
      "9   australia_assa_01  -----  2021-05-22 03:27:00  to  2021-05-22 03:28:00\n",
      "File downloaded successfully\n",
      "10   australia_assa_01  -----  2021-05-22 03:28:00  to  2021-05-22 03:29:00\n",
      "File downloaded successfully\n",
      "11   australia_assa_01  -----  2021-05-22 03:29:00  to  2021-05-22 03:30:00\n",
      "File downloaded successfully\n",
      "12   australia_assa_01  -----  2021-05-22 03:30:00  to  2021-05-22 03:31:00\n",
      "File downloaded successfully\n",
      "13   australia_assa_01  -----  2021-05-22 03:31:00  to  2021-05-22 03:32:00\n",
      "File downloaded successfully\n",
      "14   australia_assa_01  -----  2021-05-22 03:32:00  to  2021-05-22 03:33:00\n",
      "File downloaded successfully\n",
      "15   australia_assa_01  -----  2021-05-22 03:33:00  to  2021-05-22 03:34:00\n",
      "File downloaded successfully\n",
      "16   australia_assa_01  -----  2021-05-22 03:34:00  to  2021-05-22 03:35:00\n",
      "File downloaded successfully\n",
      "17   australia_assa_01  -----  2021-05-22 03:35:00  to  2021-05-22 03:36:00\n",
      "File downloaded successfully\n",
      "18   australia_assa_01  -----  2021-05-22 03:36:00  to  2021-05-22 03:37:00\n",
      "File downloaded successfully\n",
      "19   australia_assa_01  -----  2021-05-22 03:37:00  to  2021-05-22 03:38:00\n",
      "File downloaded successfully\n",
      "20   australia_assa_01  -----  2021-05-22 03:38:00  to  2021-05-22 03:39:00\n",
      "File downloaded successfully\n",
      "21   australia_assa_01  -----  2021-05-22 03:39:00  to  2021-05-22 03:40:00\n",
      "File downloaded successfully\n",
      "22   australia_assa_01  -----  2021-05-22 03:40:00  to  2021-05-22 03:41:00\n",
      "File downloaded successfully\n",
      "23   australia_assa_01  -----  2021-05-22 03:41:00  to  2021-05-22 03:42:00\n",
      "File downloaded successfully\n",
      "24   australia_assa_01  -----  2021-05-22 03:42:00  to  2021-05-22 03:43:00\n",
      "File downloaded successfully\n",
      "25   australia_assa_01  -----  2021-05-22 03:43:00  to  2021-05-22 03:44:00\n",
      "File downloaded successfully\n",
      "26   australia_assa_01  -----  2021-05-22 03:44:00  to  2021-05-22 03:45:00\n",
      "File downloaded successfully\n",
      "27   australia_assa_01  -----  2021-05-22 03:45:00  to  2021-05-22 03:46:00\n",
      "File downloaded successfully\n",
      "28   australia_assa_01  -----  2021-05-22 03:46:00  to  2021-05-22 03:47:00\n",
      "File downloaded successfully\n",
      "29   australia_assa_01  -----  2021-05-22 03:47:00  to  2021-05-22 03:48:00\n",
      "File downloaded successfully\n",
      "30   australia_assa_01  -----  2021-05-22 03:48:00  to  2021-05-22 03:49:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2022-02-08 05:39:00 to 2022-02-08 05:40:00\n",
      "Skipping australia_assa_01 from 2023-05-16 07:02:00 to 2023-05-16 07:03:00\n",
      "Skipping australia_assa_01 from 2022-12-24 04:09:00 to 2022-12-24 04:10:00\n",
      "Skipping australia_assa_01 from 2022-12-24 04:10:00 to 2022-12-24 04:11:00\n",
      "Skipping australia_assa_01 from 2022-12-24 04:11:00 to 2022-12-24 04:12:00\n",
      "31   australia_assa_01  -----  2021-09-18 01:04:00  to  2021-09-18 01:05:00\n",
      "File downloaded successfully\n",
      "32   australia_assa_01  -----  2021-09-18 01:05:00  to  2021-09-18 01:06:00\n",
      "File downloaded successfully\n",
      "33   australia_assa_01  -----  2021-09-18 01:06:00  to  2021-09-18 01:07:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2022-09-06 04:45:00 to 2022-09-06 04:46:00\n",
      "34   australia_assa_01  -----  2021-09-30 02:21:00  to  2021-09-30 02:22:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2022-10-08 01:54:00 to 2022-10-08 01:55:00\n",
      "Skipping australia_assa_01 from 2022-10-08 01:55:00 to 2022-10-08 01:56:00\n",
      "Skipping australia_assa_01 from 2023-03-03 05:52:00 to 2023-03-03 05:53:00\n",
      "Skipping australia_assa_01 from 2023-07-03 02:16:00 to 2023-07-03 02:17:00\n",
      "Skipping australia_assa_01 from 2023-01-04 06:57:00 to 2023-01-04 06:58:00\n",
      "Skipping australia_assa_01 from 2023-01-09 20:55:00 to 2023-01-09 20:56:00\n",
      "35   australia_assa_01  -----  2021-10-25 00:55:00  to  2021-10-25 00:56:00\n",
      "File downloaded successfully\n",
      "36   australia_assa_01  -----  2021-09-09 04:42:00  to  2021-09-09 04:43:00\n",
      "File downloaded successfully\n",
      "37   australia_assa_01  -----  2021-09-09 04:43:00  to  2021-09-09 04:44:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2022-03-06 07:58:00 to 2022-03-06 07:59:00\n",
      "Skipping australia_assa_01 from 2022-08-30 02:25:00 to 2022-08-30 02:26:00\n",
      "Skipping australia_assa_01 from 2022-01-11 07:39:00 to 2022-01-11 07:40:00\n",
      "38   australia_assa_01  -----  2021-09-22 22:24:00  to  2021-09-22 22:25:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2023-05-19 22:27:00 to 2023-05-19 22:28:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:28:00 to 2023-05-19 22:29:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:29:00 to 2023-05-19 22:30:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:30:00 to 2023-05-19 22:31:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:31:00 to 2023-05-19 22:32:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:32:00 to 2023-05-19 22:33:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:33:00 to 2023-05-19 22:34:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:34:00 to 2023-05-19 22:35:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:35:00 to 2023-05-19 22:36:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:36:00 to 2023-05-19 22:37:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:37:00 to 2023-05-19 22:38:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:38:00 to 2023-05-19 22:39:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:39:00 to 2023-05-19 22:40:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:40:00 to 2023-05-19 22:41:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:41:00 to 2023-05-19 22:42:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:42:00 to 2023-05-19 22:43:00\n",
      "Skipping australia_assa_01 from 2023-05-19 22:43:00 to 2023-05-19 22:44:00\n",
      "39   australia_assa_01  -----  2021-09-26 22:47:00  to  2021-09-26 22:48:00\n",
      "File downloaded successfully\n",
      "40   australia_assa_01  -----  2021-09-26 22:48:00  to  2021-09-26 22:49:00\n",
      "File downloaded successfully\n",
      "41   australia_assa_01  -----  2021-09-26 22:49:00  to  2021-09-26 22:50:00\n",
      "File downloaded successfully\n",
      "42   australia_assa_01  -----  2021-09-26 22:50:00  to  2021-09-26 22:51:00\n",
      "File downloaded successfully\n",
      "43   australia_assa_01  -----  2021-09-26 22:51:00  to  2021-09-26 22:52:00\n",
      "File downloaded successfully\n",
      "44   australia_assa_01  -----  2021-09-26 22:52:00  to  2021-09-26 22:53:00\n",
      "File downloaded successfully\n",
      "45   australia_assa_01  -----  2021-09-26 22:53:00  to  2021-09-26 22:54:00\n",
      "File downloaded successfully\n",
      "46   australia_assa_01  -----  2021-09-26 22:54:00  to  2021-09-26 22:55:00\n",
      "File downloaded successfully\n",
      "47   australia_assa_01  -----  2021-09-26 22:55:00  to  2021-09-26 22:56:00\n",
      "File downloaded successfully\n",
      "48   australia_assa_01  -----  2021-09-26 22:56:00  to  2021-09-26 22:57:00\n",
      "File downloaded successfully\n",
      "49   australia_assa_01  -----  2021-10-24 06:09:00  to  2021-10-24 06:10:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2022-12-13 07:28:00 to 2022-12-13 07:29:00\n",
      "Skipping australia_assa_01 from 2022-11-06 01:17:00 to 2022-11-06 01:18:00\n",
      "Skipping australia_assa_01 from 2022-11-06 01:18:00 to 2022-11-06 01:19:00\n",
      "Skipping australia_assa_01 from 2022-11-06 01:19:00 to 2022-11-06 01:20:00\n",
      "Skipping australia_assa_01 from 2022-11-06 01:20:00 to 2022-11-06 01:21:00\n",
      "Skipping australia_assa_01 from 2022-11-06 01:21:00 to 2022-11-06 01:22:00\n",
      "Skipping australia_assa_01 from 2022-11-06 01:22:00 to 2022-11-06 01:23:00\n",
      "Skipping australia_assa_01 from 2022-11-06 01:23:00 to 2022-11-06 01:24:00\n",
      "50   australia_assa_01  -----  2021-09-01 05:33:00  to  2021-09-01 05:34:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2022-10-11 02:45:00 to 2022-10-11 02:46:00\n",
      "Skipping australia_assa_01 from 2022-10-11 02:46:00 to 2022-10-11 02:47:00\n",
      "Skipping australia_assa_01 from 2022-10-11 02:47:00 to 2022-10-11 02:48:00\n",
      "Skipping australia_assa_01 from 2022-10-11 02:48:00 to 2022-10-11 02:49:00\n",
      "Skipping australia_assa_01 from 2022-10-11 02:49:00 to 2022-10-11 02:50:00\n",
      "Skipping australia_assa_01 from 2022-10-11 02:50:00 to 2022-10-11 02:51:00\n",
      "Skipping australia_assa_01 from 2022-10-11 02:51:00 to 2022-10-11 02:52:00\n",
      "Skipping australia_assa_01 from 2022-10-11 02:52:00 to 2022-10-11 02:53:00\n",
      "51   australia_assa_01  -----  2021-05-22 06:05:00  to  2021-05-22 06:06:00\n",
      "File downloaded successfully\n",
      "Skipping australia_assa_01 from 2023-01-09 04:39:00 to 2023-01-09 04:40:00\n",
      "Skipping australia_assa_01 from 2022-02-05 20:44:00 to 2022-02-05 20:45:00\n",
      "Skipping australia_assa_01 from 2023-04-28 03:13:00 to 2023-04-28 03:14:00\n",
      "Skipping australia_assa_01 from 2023-03-15 07:29:00 to 2023-03-15 07:30:00\n",
      "52   australia_assa_01  -----  2021-09-30 04:36:00  to  2021-09-30 04:37:00\n",
      "File downloaded successfully\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m check_table_data_availability(instrument_table, \u001b[39mstr\u001b[39m(date), \u001b[39mstr\u001b[39m(end_date)):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# Attempt to retrieve the data and save it as an image\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39m# Parameters: instrument_table, start date, end date, x-limits, y-limits, burst category, data type\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(image_num, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, instrument_table, \u001b[39m\"\u001b[39m\u001b[39m ----- \u001b[39m\u001b[39m\"\u001b[39m, date, \u001b[39m\"\u001b[39m\u001b[39m to \u001b[39m\u001b[39m\"\u001b[39m, end_date)\n\u001b[1;32m---> 24\u001b[0m     get_data_save_as_img(\n\u001b[0;32m     25\u001b[0m         instrument\u001b[39m=\u001b[39;49minstrument_table, \n\u001b[0;32m     26\u001b[0m         start_datetime\u001b[39m=\u001b[39;49mdate, \n\u001b[0;32m     27\u001b[0m         end_datetime\u001b[39m=\u001b[39;49mend_date, \n\u001b[0;32m     28\u001b[0m         time_bucket\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \n\u001b[0;32m     29\u001b[0m         agg_function\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     30\u001b[0m         burst_type\u001b[39m=\u001b[39;49mburst_category,\n\u001b[0;32m     31\u001b[0m         min_shape\u001b[39m=\u001b[39;49m(\u001b[39m200\u001b[39;49m, \u001b[39m200\u001b[39;49m),\n\u001b[0;32m     32\u001b[0m         data_folder\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     33\u001b[0m     )\n\u001b[0;32m     34\u001b[0m     image_num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m image_num \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m TOTAL_IMAGE_NUM:\n",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m, in \u001b[0;36mget_data_save_as_img\u001b[1;34m(instrument, start_datetime, end_datetime, time_bucket, agg_function, burst_type, min_shape, data_folder)\u001b[0m\n\u001b[0;32m     27\u001b[0m sd_str \u001b[39m=\u001b[39m start_datetime\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m ed_str \u001b[39m=\u001b[39m end_datetime\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m df \u001b[39m=\u001b[39m get_data(\n\u001b[0;32m     30\u001b[0m     instrument_name\u001b[39m=\u001b[39;49minstrument, \n\u001b[0;32m     31\u001b[0m     start_datetime\u001b[39m=\u001b[39;49msd_str,\n\u001b[0;32m     32\u001b[0m     end_datetime\u001b[39m=\u001b[39;49med_str, \n\u001b[0;32m     33\u001b[0m     timebucket\u001b[39m=\u001b[39;49mtime_bucket, \n\u001b[0;32m     34\u001b[0m     agg_function\u001b[39m=\u001b[39;49magg_function\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     37\u001b[0m img_data \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint16)\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m img_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_shape[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m img_data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_shape[\u001b[39m1\u001b[39m]: \n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\ecallisto_ng\\data_fetching\\get_data.py:93\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(instrument_name, start_datetime, end_datetime, timebucket, agg_function, data_folder, verbose)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     92\u001b[0m         f\u001b[39m.\u001b[39mwrite(file_response\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mread_parquet(file_path)\n\u001b[0;32m     94\u001b[0m \u001b[39melif\u001b[39;00m file_response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m204\u001b[39m:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# Check if the file creation causes any errors\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# This json contains information about the request\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     json_response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(BASE_URL \u001b[39m+\u001b[39m json_url)\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\pandas\\io\\parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 509\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[0;32m    510\u001b[0m     path,\n\u001b[0;32m    511\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m    512\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    513\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[0;32m    514\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    515\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    516\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\pandas\\io\\parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    221\u001b[0m     path,\n\u001b[0;32m    222\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    223\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m    224\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mread_table(\n\u001b[0;32m    228\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    229\u001b[0m     )\n\u001b[0;32m    230\u001b[0m     result \u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2939\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2932\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2933\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword is no longer supported with the new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2934\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets-based implementation. Specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2935\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to temporarily recover the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2936\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbehaviour.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2937\u001b[0m     )\n\u001b[0;32m   2938\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2939\u001b[0m     dataset \u001b[39m=\u001b[39m _ParquetDatasetV2(\n\u001b[0;32m   2940\u001b[0m         source,\n\u001b[0;32m   2941\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m   2942\u001b[0m         filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m   2943\u001b[0m         partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[0;32m   2944\u001b[0m         memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[0;32m   2945\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary,\n\u001b[0;32m   2946\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size,\n\u001b[0;32m   2947\u001b[0m         filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[0;32m   2948\u001b[0m         ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes,\n\u001b[0;32m   2949\u001b[0m         pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[0;32m   2950\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[0;32m   2951\u001b[0m         thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[0;32m   2952\u001b[0m         thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[0;32m   2953\u001b[0m     )\n\u001b[0;32m   2954\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m   2955\u001b[0m     \u001b[39m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m     \u001b[39m# module is not available\u001b[39;00m\n\u001b[0;32m   2957\u001b[0m     \u001b[39mif\u001b[39;00m filters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2479\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[1;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m single_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2476\u001b[0m     fragment \u001b[39m=\u001b[39m parquet_format\u001b[39m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[0;32m   2478\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mFileSystemDataset(\n\u001b[1;32m-> 2479\u001b[0m         [fragment], schema\u001b[39m=\u001b[39mschema \u001b[39mor\u001b[39;00m fragment\u001b[39m.\u001b[39;49mphysical_schema,\n\u001b[0;32m   2480\u001b[0m         \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mparquet_format,\n\u001b[0;32m   2481\u001b[0m         filesystem\u001b[39m=\u001b[39mfragment\u001b[39m.\u001b[39mfilesystem\n\u001b[0;32m   2482\u001b[0m     )\n\u001b[0;32m   2483\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   2485\u001b[0m \u001b[39m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\pyarrow\\_dataset.pyx:1345\u001b[0m, in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bbaki\\miniconda3\\envs\\fhnw\\Lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes"
     ]
    }
   ],
   "source": [
    "# Iterate through each row in the filtered burst list\n",
    "for index, burst_row in burst_list_filtered.sample(frac=1).iterrows():\n",
    "    \n",
    "    # Iterate through each instrument table to include\n",
    "    for instrument_table in INSTRUMENTS_TO_INCLUDE:\n",
    "        \n",
    "        # Get the start and end datetime of the burst\n",
    "        burst_start = burst_row.datetime_start\n",
    "        burst_end = burst_row.datetime_end \n",
    "        \n",
    "        # Create a date range from the start to end datetime with a frequency of IMAGE_LENGTH, including the left endpoint\n",
    "        burst_date_range = pd.date_range(burst_start, burst_end, freq=IMAGE_LENGTH, inclusive='left')\n",
    "        \n",
    "        # Get the burst type as a string\n",
    "        burst_category = str(burst_row.type)\n",
    "        \n",
    "        # Retrieve data for each date in the burst_date_range\n",
    "        for date in burst_date_range:\n",
    "            end_date = date + timedelta(minutes=1)\n",
    "            if check_table_data_availability(instrument_table, str(date), str(end_date)):\n",
    "                # Attempt to retrieve the data and save it as an image\n",
    "                # Parameters: instrument_table, start date, end date, x-limits, y-limits, burst category, data type\n",
    "                print(image_num, \" \", instrument_table, \" ----- \", date, \" to \", end_date)\n",
    "                get_data_save_as_img(\n",
    "                    instrument=instrument_table, \n",
    "                    start_datetime=date, \n",
    "                    end_datetime=end_date, \n",
    "                    time_bucket=None, \n",
    "                    agg_function=None,\n",
    "                    burst_type=burst_category,\n",
    "                    min_shape=(200, 200),\n",
    "                    data_folder='data'\n",
    "                )\n",
    "                image_num += 1\n",
    "                if image_num >= TOTAL_IMAGE_NUM:\n",
    "                    break\n",
    "            else:                \n",
    "                print(f\"Skipping {instrument_table} from {date} to {date + timedelta(minutes=1)}\")\n",
    "                \n",
    "        if image_num >= TOTAL_IMAGE_NUM:\n",
    "            break\n",
    "    if image_num >= TOTAL_IMAGE_NUM:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create images of non burst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOW_MANY_IMAGES = 50\n",
    "###\n",
    "MIN_START_TIME = burst_list_filtered.datetime_start.min() #burst_list_filtered.datetime_start.apply(lambda dt: dt.replace(hour=8, minute=0, second=0)).min()\n",
    "MAX_START_TIME = burst_list_filtered.datetime_start.max() - IMAGE_LENGTH #MIN_START_TIME + timedelta(hours=12) - IMAGE_LENGTH\n",
    "\n",
    "print(MIN_START_TIME)\n",
    "print(\"----\")\n",
    "print(MAX_START_TIME)\n",
    "def random_date(start, end):\n",
    "    \"\"\"Generate a random datetime between `start` and `end`\"\"\"\n",
    "    return start + timedelta(\n",
    "        # Get a random amount of seconds between `start` and `end`\n",
    "        minutes=random.randint(0, int((end - start).total_seconds() // 60)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for the number of images added\n",
    "images_processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue processing until the required number of images is reached\n",
    "while images_processed < HOW_MANY_IMAGES:\n",
    "    # Generate a random start date between a minimum and maximum start time\n",
    "    random_start_date = random_date(MIN_START_TIME, MAX_START_TIME)\n",
    "\n",
    "    # Iterate through each instrument table to include\n",
    "    for instrument_table in INSTRUMENTS_TO_INCLUDE:\n",
    "        \n",
    "        # Remove ID from the instrument name to get the base name\n",
    "        base_instrument_name = remove_id_from_instrument_name(instrument_table)\n",
    "        \n",
    "        # Filter the burst list for entries that match the current instrument's base name\n",
    "        burst_list_for_instrument = burst_list_filtered[burst_list_filtered.instruments == base_instrument_name]\n",
    "        \n",
    "        # If the random start date falls within any burst period for the current instrument, continue to next iteration\n",
    "        if any((burst_list_for_instrument.datetime_start <= random_start_date) & (random_start_date <= burst_list_for_instrument.datetime_end)):\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                # Attempt to retrieve the data and save it as an image\n",
    "                # Parameters: instrument_table, start date, end date, x-limits, y-limits, burst category, data type\n",
    "                get_data_save_as_img(\n",
    "                    instrument_table, \n",
    "                    random_start_date, \n",
    "                    random_start_date + timedelta(minutes=1), \n",
    "                    time_bucket=None, \n",
    "                    agg_function=None,\n",
    "                    burst_type='no_burst',\n",
    "                    min_shape=(200, 200),\n",
    "                    data_folder='data'\n",
    "                )\n",
    "                # Increment the count of images processed\n",
    "                images_processed += 1\n",
    "            except ValueError as e:\n",
    "                # Handle exception: print the error and skip to next date\n",
    "                print(e)\n",
    "                print(f\"Skipping {instrument_table} from {random_start_date} to {random_start_date + timedelta(minutes=1)}\")\n",
    "                pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radio_sunburst_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
